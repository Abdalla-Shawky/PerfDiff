# Performance Regression Detection Tool

A production-ready statistical tool for detecting performance regressions with **premium UI**, **data quality gates**, and **rigorous statistical methodology**. Features world-class HTML reports, interactive visualizations, and automatic reliability checks.

[![Tests](https://img.shields.io/badge/tests-52%2F52%20passing-success)](docs/TEST_REPORT.md)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-green)]()
[![Statistical](https://img.shields.io/badge/statistical-rigorous-purple)]()

---

## ðŸ“‘ Table of Contents

- [ðŸŽ¯ Why This Tool Exists](#-why-this-tool-exists)
- [âœ… What This Tool Does Differently](#-what-this-tool-does-differently)
- [ðŸš€ Quick Start](#-quick-start)
- [ðŸ“– Regression Detection Gates](#-regression-detection-gates)
- [ðŸ“Š Comparison Table](#-comparison-table)
- [ðŸ’¡ Usage Examples](#-usage-examples)
- [ðŸ§ª Running Tests](#-running-tests)
- [ðŸŽ¯ Who Is This Tool For?](#-who-is-this-tool-for)
- [ðŸš¦ Configuration Options](#-configuration-options)
- [ðŸ’¡ Quick Tips](#-quick-tips)
- [ðŸ“Š Project Structure](#-project-structure)
- [ðŸ† Why This Tool?](#-why-this-tool)
- [ðŸ“š Documentation](#-documentation)
- [ðŸ“„ License](#-license)

---

## ðŸŽ¯ Why This Tool Exists

### The Performance Testing Problem

Every engineering team faces these challenges when testing performance:

| Problem | What Happens | Impact |
|---------|--------------|--------|
| **False Positives** | 2ms noise flagged as "regression" | âŒ Blocked PRs, wasted time |
| **False Negatives** | Real 50ms regression missed due to variance | âŒ Regressions reach production |
| **No Statistical Rigor** | "Is 5ms real or noise?" â†’ Unknown | âŒ Guesswork, not data |
| **Poor Data Quality** | 5 runs with 40% variance = "valid" | âŒ Unreliable conclusions |
| **Tail Latency Ignored** | Median OK, but P90 +200ms | âŒ User pain not detected |
| **Arbitrary Thresholds** | Fixed 10ms threshold for all operations | âŒ Fails for fast ops, misses slow ops |

### What Teams Usually Do (Wrong)

```python
# âŒ The naive approach
baseline_median = median(baseline_runs)
target_median = median(target_runs)

if target_median > baseline_median:
    print("REGRESSION!")  # False positives everywhere
```

**Problems:**
- No confidence: Is the difference real or random noise?
- No quality check: Works with any garbage data
- No tail check: Misses worst-case performance
- No context: Same threshold for 10ms and 10s operations

---

## âœ… What This Tool Does Differently

This tool was built from the ground up with **statistical rigor** to solve real performance testing problems.

### 1. **Multi-Layered Defense Against False Results**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Quality Gates   â”‚  â† Reject bad data FIRST
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
       â”‚ Median      â”‚ â”‚ Tail    â”‚ â”‚ Direction   â”‚
       â”‚ Threshold   â”‚ â”‚ Latency â”‚ â”‚ Check       â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â”‚              â”‚              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Mann-Whitney U   â”‚ â† Statistical test
                    â”‚ (One-sided)      â”‚   (directional)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Practical        â”‚ â† Prevent false
                    â”‚ Significance     â”‚   positives on
                    â”‚ Override         â”‚   tiny changes
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this matters:**
- **Quality gates** prevent garbage-in-garbage-out
- **Multiple checks** catch different types of regressions
- **Statistical test** provides confidence (p-value)
- **Practical override** prevents false positives on negligible changes

### 2. **Statistical Rigor (Not Guesswork)**

| What It Does | Why It Matters | Example |
|--------------|----------------|---------|
| **Mann-Whitney U Test** (one-sided) | Proves difference is real, not noise | p=0.003 â†’ 99.7% confident target is slower |
| **Bootstrap CI** | Quantifies uncertainty | 95% CI: [8ms, 22ms] â†’ won't include 0 if real |
| **Direction Checks** | Never fails on improvements | P(T>B) > 50% AND median_delta > 0 AND p < 0.05 |
| **Adaptive Tail Metric** | Stable with small samples | Mean of worst k samples (k adaptive) |
| **Quality Gates** | Rejects unreliable data | CV > 15% â†’ INCONCLUSIVE (not false result) |

### 3. **Solves Real Problems**

**Problem 1: False Positive on Noise**
```
Baseline: 2400ms, Target: 2402.5ms
Simple median: "REGRESSION!" âŒ
This tool: "PASS - below practical threshold (20ms)" âœ…
```

**Problem 2: Missed Tail Regression**
```
Baseline: Median 101ms, Tail 150ms
Target:   Median 101ms, Tail 350ms
Simple median: "No change" âŒ
This tool: "FAIL - Tail delta 200ms exceeds threshold" âœ…
```

**Problem 3: Garbage Data**
```
Runs: [100, 95, 180, 90, 85]  # One wild outlier
CV = 34.5%
Simple median: "Valid result" âŒ
This tool: "INCONCLUSIVE - CV 34.5% > 15% max" âœ…
```

**Problem 4: False Failure on Improvement**
```
Target is 20ms faster (improvement!)
Old tools: Could fail due to statistical significance
This tool: "PASS" (direction check prevents false failures) âœ…
```

---

## ðŸš€ Quick Start

### Installation

```bash
# Method 1: Install from GitHub (recommended)
pip install git+https://github.com/Abdalla-Shawky/PerfDiff.git@v1.0.0

# Method 2: Local development installation
git clone https://github.com/Abdalla-Shawky/PerfDiff.git
cd PerfDiff
pip install -e .

# Verify installation
perfdiff --help
```

**Dependencies (auto-installed):**
- `numpy` (â‰¥1.20.0) - Numerical computing
- `scipy` (â‰¥1.7.0) - Statistical functions (Mann-Whitney U test)

### Try It Now (Mock Data)

```bash
# Using CLI command
perfdiff \
    commit2commit/mock_data/baseline_traces.json \
    commit2commit/mock_data/target_traces.json \
    --output-dir ./test_output

# Or using module directly
python -m commit2commit.multi_trace_comparison \
    commit2commit/mock_data/baseline_traces.json \
    commit2commit/mock_data/target_traces.json \
    --output-dir ./test_output

# View results
open test_output/index.html
```

**Output:**
- ðŸ“Š `test_output/index.html` - Summary of all traces
- ðŸ“„ `test_output/[trace_name].html` - Detailed report per trace
- ðŸš¦ Exit code: 0 (PASS) or 1 (FAIL with regressions)

---

## ðŸ“– Regression Detection Gates

The tool performs checks in this order:

### 1. Quality Gates (Pre-check)
**Purpose:** Reject unreliable data before analysis
- Sample size: n â‰¥ 10 required
- Coefficient of variation: CV â‰¤ 15% required
- **Result:** PASS/INCONCLUSIVE (never FAIL on quality alone)

### 2. Median Delta Check
**Purpose:** Detect median performance change
- Threshold: max(5ms, 3% of baseline)
- No CV adjustment (CV > 15% â†’ INCONCLUSIVE instead)
- **Result:** PASS/FAIL

### 3. Tail Latency Check
**Purpose:** Detect worst-case performance degradation
- Metric: Adaptive trimmed mean of worst k samples (k = 2 to 5)
- Threshold: max(75ms, 5% of baseline tail)
- No CV adjustment (CV > 15% â†’ INCONCLUSIVE instead)
- **Result:** PASS/FAIL

### 4. Directionality (Informational)
**Purpose:** Screening metric only
- Metric: Fraction of target samples > baseline median
- Stored in details, NOT used for PASS/FAIL
- Mann-Whitney P(T>B) is the confirmatory test

### 5. Mann-Whitney U Test (One-sided)
**Purpose:** Statistical significance test
- Test: One-sided, alternative='greater'
- Alpha: 0.05 (5% significance level)
- **Direction check:** p < 0.05 AND P(T>B) >= 0.55
- **Note:** Removed `median_delta > 0` to catch tail-only regressions
- **Result:** PASS/FAIL

### 6. Practical Significance Override (Post-check)
**Purpose:** Prevent false positives on negligible changes
- **Dual threshold:** median_delta < threshold AND tail_delta < tail_threshold
- Overrides statistical failures when changes are negligible
- **Result:** Can convert FAIL â†’ PASS (with explanation)

---

## ðŸ“Š Comparison Table

| Aspect | Simple Median | This Tool |
|--------|---------------|-----------|
| **Data quality check** | None | Quality gates (CV â‰¤ 15%, n â‰¥ 10) |
| **Threshold type** | Fixed or relative only | Adaptive (max of absolute + relative) |
| **Tail latency** | Not checked | Adaptive trimmed mean (k scales with n) |
| **Consistency check** | None | Directionality (informational) |
| **Statistical test** | None | One-sided Mann-Whitney U test |
| **Direction check** | None | Triple condition (prevents false failures) |
| **False positives** | High (2ms noise = "regression") | Low (practical significance override) |
| **False negatives** | High (noise hides real issues) | Low (multiple detection layers) |
| **Uncertainty** | Unknown | Bootstrap confidence intervals |
| **Result types** | Pass/Fail | Pass/Fail/Inconclusive/No Change |
| **Small sample handling** | Unreliable | Adaptive tail metric (stable with n=12) |
| **Multiple testing** | Not addressed | Documented (only 1 p-value test) |

---

## ðŸ’¡ Usage Examples

### CLI Usage

```bash
# Production use
perfdiff baseline.json target.json --output-dir ./reports
```

### Programmatic Usage

```python
from commit2commit.trace_to_trace import gate_regression
import numpy as np

baseline = np.array([100, 102, 98, 101, 99, 103, 97, 100, 102, 101])
target = np.array([110, 112, 108, 111, 109, 113, 107, 110, 112, 111])

result = gate_regression(baseline, target)

if not result.passed:
    print(f"âŒ {result.reason}")
    print(f"Î” median: {result.details['median_delta_ms']}ms")
    print(f"p-value: {result.details['mann_whitney_p']:.4f}")
```

### CI/CD Integration

```bash
pip install git+https://github.com/Abdalla-Shawky/PerfDiff.git@v1.0.0
perfdiff baseline.json target.json --output-dir ./reports
[ $? -eq 1 ] && echo "âŒ Regressions detected" && exit 1
```

---

## ðŸ§ª Running Tests

```bash
# Install test dependencies
pip install pytest

# Run all tests
cd commit2commit
python -m pytest test_trace_to_trace.py -v

# Expected output:
# ========================= 52 passed in ~5.0s =========================
```

---

## ðŸŽ¯ Who Is This Tool For?

**Perfect for:**
- ðŸ”§ **Performance Engineers** - Validating optimizations with statistical rigor
- âœ… **QA Teams** - Setting up reliable performance gates
- ðŸ‘¥ **Engineering Teams** - Tracking performance trends over time
- ðŸ—ï¸ **Platform Teams** - Monitoring system health
- ðŸ”¬ **Data Scientists** - Anyone who values statistical correctness

**Not just another perf tool. This is:**
- âœ… Statistically rigorous (Mann-Whitney U, Bootstrap CI)
- âœ… Battle-tested (52/52 tests passing)
- âœ… Production-ready (comprehensive documentation)
- âœ… Professional UI (stakeholders trust it)
- âœ… False-positive resistant (practical significance override)
- âœ… False-negative resistant (multiple detection layers)

---

## ðŸš¦ Configuration Options

### Quick Reference

| Option | Description | Default |
|--------|-------------|---------|
| `--baseline` | Baseline measurements (required) | - |
| `--target` | Target measurements (required) | - |
| `--out` | Output HTML file (required) | - |
| `--mode` | `pr` or `release` | `pr` |
| `--ms-floor` | Absolute threshold (ms) | `5.0` |
| `--pct-floor` | Relative threshold (fraction) | `0.03` (3%) |
| `--tail-ms-floor` | Tail absolute threshold | `75.0` |
| `--tail-pct-floor` | Tail relative threshold | `0.05` (5%) |
| `--directionality` | Max fraction slower (informational) | `0.70` (70%) |
| `--no-mann-whitney` | Disable Mann-Whitney U test | False |
| `--mann-whitney-alpha` | Significance level | `0.05` |
| `--seed` | Random seed | `0` |

See `--help` for all options.

---

## ðŸ’¡ Quick Tips

### Choosing the Right Mode

**PR Mode** (`--mode pr`): Strict regression gate
- Use for: PRs, feature branches, continuous testing
- Goal: Catch any performance degradation
- Fails if: Median, tail, or Mann-Whitney exceeds thresholds

**Release Mode** (`--mode release`): Equivalence testing
- Use for: Release validation, stable builds
- Goal: Ensure performance hasn't significantly changed
- Fails if: Bootstrap CI for median delta is outside margin

### Sample Size Guidelines

| Samples | Reliability | Use Case |
|---------|-------------|----------|
| 3-5 | Low | Quick checks only |
| 10-20 | Good | Most use cases âœ… |
| 30+ | Excellent | Critical paths, noisy environments |

### Sequential Testing Methodology

âœ… **Correct (AAA BBB):**
```bash
# Run all baseline measurements
for i in {1..10}; do measure_baseline; done
# Then run all target measurements
for i in {1..10}; do measure_target; done
```

âŒ **Incorrect (interleaved):**
```bash
# Don't interleave measurements
for i in {1..10}; do
  measure_baseline
  measure_target
done
```

**Why?** The tool uses independent samples testing (Mann-Whitney U), which assumes samples are collected independently.

---

## ðŸ“Š Project Structure

```
PerfDiff/
â”œâ”€â”€ commit2commit/                          # Main package
â”‚   â”œâ”€â”€ trace_to_trace.py                  # Core: Single trace statistical analysis
â”‚   â”œâ”€â”€ multi_trace_comparison.py          # Orchestrator: Multi-trace + CLI entry point
â”‚   â”œâ”€â”€ constants.py                       # Configuration thresholds
â”‚   â”‚
â”‚   â”œâ”€â”€ perf_html_report.py                # HTML report generator
â”‚   â”œâ”€â”€ perf_html_template.py              # Premium UI template
â”‚   â”œâ”€â”€ comparison_html_template.py        # Summary table template
â”‚   â”œâ”€â”€ trace_detail_html_template.py      # Individual trace template
â”‚   â”œâ”€â”€ timeline_html_template.py          # Timeline visualization
â”‚   â”‚
â”‚   â”œâ”€â”€ test_trace_to_trace.py             # Test suite (52 tests passing)
â”‚   â”œâ”€â”€ mock_data/                         # Sample traces for testing
â”‚   â”‚   â”œâ”€â”€ baseline_traces.json
â”‚   â”‚   â””â”€â”€ target_traces.json
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ setup.py                                # pip installation config
â”œâ”€â”€ requirements.txt                        # Python dependencies
â”œâ”€â”€ run_comparison.sh                       # Quick test script
â”‚
â”œâ”€â”€ README.md                               # This file
â”œâ”€â”€ STATISTICAL_FIXES_SUMMARY.md           # Statistical methodology
â”œâ”€â”€ TOOL_TECHNICAL_SUMMARY.md              # Technical details
â”œâ”€â”€ EXECUTIVE_SUMMARY.md                   # High-level overview
â”‚
â””â”€â”€ docs/                                   # Detailed documentation
    â”œâ”€â”€ USER_GUIDE.md
    â”œâ”€â”€ TEST_REPORT.md
    â””â”€â”€ ... (20+ documentation files)
```

**Key Modules:**

| Module | Purpose |
|--------|---------|
| `trace_to_trace.py` | Core statistical engine - compares one trace pair |
| `multi_trace_comparison.py` | CLI tool - compares multiple traces, generates reports |
| `constants.py` | All thresholds (MS_FLOOR, PCT_FLOOR, CV limits, etc.) |
| `perf_html_*.py` | HTML report generation with interactive charts |

---
## ðŸ“š Documentation

ðŸ“– **Full User Guide**: [docs/USER_GUIDE.md](docs/USER_GUIDE.md)
ðŸ“Š **Technical Summary**: [TOOL_TECHNICAL_SUMMARY.md](TOOL_TECHNICAL_SUMMARY.md)
ðŸ“Š **Statistical Details**: [STATISTICAL_FIXES_SUMMARY.md](STATISTICAL_FIXES_SUMMARY.md)
ðŸ§ª **Test Results**: [docs/TEST_REPORT.md](docs/TEST_REPORT.md)
ðŸ“‹ **Executive Summary**: [EXECUTIVE_SUMMARY.md](EXECUTIVE_SUMMARY.md)

**Version:** 1.0.0
**Author:** Shawky
**Repository:** [github.com/Abdalla-Shawky/PerfDiff](https://github.com/Abdalla-Shawky/PerfDiff)

---

## ðŸ“„ License

MIT License - Feel free to use in your projects!

