# Performance Regression Detection Tool

A production-ready statistical tool for detecting performance regressions with **premium UI**, **data quality gates**, and **rigorous statistical methodology**. Features world-class HTML reports, interactive visualizations, and automatic reliability checks.

[![Tests](https://img.shields.io/badge/tests-52%2F52%20passing-success)](docs/TEST_REPORT.md)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-green)]()
[![Statistical](https://img.shields.io/badge/statistical-rigorous-purple)]()

---

## ðŸŽ¯ Why This Tool Exists

### The Performance Testing Problem

Every engineering team faces these challenges when testing performance:

| Problem | What Happens | Impact |
|---------|--------------|--------|
| **False Positives** | 2ms noise flagged as "regression" | âŒ Blocked PRs, wasted time |
| **False Negatives** | Real 50ms regression missed due to variance | âŒ Regressions reach production |
| **No Statistical Rigor** | "Is 5ms real or noise?" â†’ Unknown | âŒ Guesswork, not data |
| **Poor Data Quality** | 5 runs with 40% variance = "valid" | âŒ Unreliable conclusions |
| **Tail Latency Ignored** | Median OK, but P90 +200ms | âŒ User pain not detected |
| **Arbitrary Thresholds** | Fixed 10ms threshold for all operations | âŒ Fails for fast ops, misses slow ops |

### What Teams Usually Do (Wrong)

```python
# âŒ The naive approach
baseline_median = median(baseline_runs)
target_median = median(target_runs)

if target_median > baseline_median:
    print("REGRESSION!")  # False positives everywhere
```

**Problems:**
- No confidence: Is the difference real or random noise?
- No quality check: Works with any garbage data
- No tail check: Misses worst-case performance
- No context: Same threshold for 10ms and 10s operations

---

## âœ… What This Tool Does Differently

This tool was built from the ground up with **statistical rigor** to solve real performance testing problems.

### 1. **Multi-Layered Defense Against False Results**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Quality Gates   â”‚  â† Reject bad data FIRST
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
       â”‚ Median      â”‚ â”‚ Tail    â”‚ â”‚ Direction   â”‚
       â”‚ Threshold   â”‚ â”‚ Latency â”‚ â”‚ Check       â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â”‚              â”‚              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Mann-Whitney U   â”‚ â† Statistical test
                    â”‚ (One-sided)      â”‚   (directional)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Practical        â”‚ â† Prevent false
                    â”‚ Significance     â”‚   positives on
                    â”‚ Override         â”‚   tiny changes
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why this matters:**
- **Quality gates** prevent garbage-in-garbage-out
- **Multiple checks** catch different types of regressions
- **Statistical test** provides confidence (p-value)
- **Practical override** prevents false positives on negligible changes

### 2. **Statistical Rigor (Not Guesswork)**

| What It Does | Why It Matters | Example |
|--------------|----------------|---------|
| **Mann-Whitney U Test** (one-sided) | Proves difference is real, not noise | p=0.003 â†’ 99.7% confident target is slower |
| **Bootstrap CI** | Quantifies uncertainty | 95% CI: [8ms, 22ms] â†’ won't include 0 if real |
| **Direction Checks** | Never fails on improvements | P(T>B) > 50% AND median_delta > 0 AND p < 0.05 |
| **Adaptive Tail Metric** | Stable with small samples | Mean of worst k samples (k adaptive) |
| **Quality Gates** | Rejects unreliable data | CV > 15% â†’ INCONCLUSIVE (not false result) |

### 3. **Solves Real Problems**

**Problem 1: False Positive on Noise**
```
Baseline: 2400ms, Target: 2402.5ms
Simple median: "REGRESSION!" âŒ
This tool: "PASS - below practical threshold (20ms)" âœ…
```

**Problem 2: Missed Tail Regression**
```
Baseline: Median 101ms, Tail 150ms
Target:   Median 101ms, Tail 350ms
Simple median: "No change" âŒ
This tool: "FAIL - Tail delta 200ms exceeds threshold" âœ…
```

**Problem 3: Garbage Data**
```
Runs: [100, 95, 180, 90, 85]  # One wild outlier
CV = 34.5%
Simple median: "Valid result" âŒ
This tool: "INCONCLUSIVE - CV 34.5% > 15% max" âœ…
```

**Problem 4: False Failure on Improvement**
```
Target is 20ms faster (improvement!)
Old tools: Could fail due to statistical significance
This tool: "PASS" (direction check prevents false failures) âœ…
```

---

## ðŸš€ Quick Start

### Installation

```bash
# Method 1: Install from GitHub (recommended)
pip install git+https://github.com/Abdalla-Shawky/PerfDiff.git@v1.0.0

# Method 2: Local development installation
git clone https://github.com/Abdalla-Shawky/PerfDiff.git
cd PerfDiff
pip install -e .

# Verify installation
perfdiff --help
```

**Dependencies (auto-installed):**
- `numpy` (â‰¥1.20.0) - Numerical computing
- `scipy` (â‰¥1.7.0) - Statistical functions (Mann-Whitney U test)

### Try It Now (Mock Data)

```bash
# Using CLI command
perfdiff \
    commit2commit/mock_data/baseline_traces.json \
    commit2commit/mock_data/target_traces.json \
    --output-dir ./test_output

# Or using module directly
python -m commit2commit.multi_trace_comparison \
    commit2commit/mock_data/baseline_traces.json \
    commit2commit/mock_data/target_traces.json \
    --output-dir ./test_output

# View results
open test_output/index.html
```

This will:
- âœ… Compare multiple performance traces (baseline vs target)
- âœ… Generate interactive HTML reports with charts
- âœ… Demonstrate all regression detection features
- âœ… Exit code 0 (PASS) or 1 (FAIL with regressions detected)

### Usage Examples

**Compare multiple traces (recommended):**
```bash
# Create traces JSON files with your performance data
perfdiff baseline_traces.json target_traces.json --output-dir ./reports

# Output:
# - reports/index.html (summary of all traces)
# - reports/trace_name.html (detailed report for each trace)
```

**Single trace comparison (for custom scripts):**
```bash
python -c "
from commit2commit.trace_to_trace import gate_regression
import numpy as np

baseline = np.array([800, 805, 798, 810, 799, 803, 801, 807, 802, 804])
target = np.array([845, 850, 838, 860, 842, 848, 844, 855, 849, 847])

result = gate_regression(baseline, target)
print(f'Verdict: {\"FAIL\" if not result.passed else \"PASS\"}')
print(f'Reason: {result.reason}')
"
```

**Expected output:**
- ðŸ“„ HTML reports with interactive visualizations
- ðŸ“Š Statistical analysis (Mann-Whitney U, Bootstrap CIs)
- ðŸš¦ Exit codes: 0 (PASS), 1 (FAIL), 2+ (ERROR)

---

## ðŸ“– Regression Detection Gates

The tool performs checks in this order:

### 1. Quality Gates (Pre-check)
**Purpose:** Reject unreliable data before analysis
- Sample size: n â‰¥ 10 required
- Coefficient of variation: CV â‰¤ 15% required
- **Result:** PASS/INCONCLUSIVE (never FAIL on quality alone)

### 2. Median Delta Check
**Purpose:** Detect median performance change
- Threshold: max(5ms, 3% of baseline)
- No CV adjustment (CV > 15% â†’ INCONCLUSIVE instead)
- **Result:** PASS/FAIL

### 3. Tail Latency Check
**Purpose:** Detect worst-case performance degradation
- Metric: Adaptive trimmed mean of worst k samples (k = 2 to 5)
- Threshold: max(75ms, 5% of baseline tail)
- No CV adjustment (CV > 15% â†’ INCONCLUSIVE instead)
- **Result:** PASS/FAIL

### 4. Directionality (Informational)
**Purpose:** Screening metric only
- Metric: Fraction of target samples > baseline median
- Stored in details, NOT used for PASS/FAIL
- Mann-Whitney P(T>B) is the confirmatory test

### 5. Mann-Whitney U Test (One-sided)
**Purpose:** Statistical significance test
- Test: One-sided, alternative='greater'
- Alpha: 0.05 (5% significance level)
- **Direction check:** p < 0.05 AND P(T>B) >= 0.55
- **Note:** Removed `median_delta > 0` to catch tail-only regressions
- **Result:** PASS/FAIL

### 6. Practical Significance Override (Post-check)
**Purpose:** Prevent false positives on negligible changes
- **Dual threshold:** median_delta < threshold AND tail_delta < tail_threshold
- Overrides statistical failures when changes are negligible
- **Result:** Can convert FAIL â†’ PASS (with explanation)

---

## ðŸ“Š Comparison Table

| Aspect | Simple Median | This Tool |
|--------|---------------|-----------|
| **Data quality check** | None | Quality gates (CV â‰¤ 15%, n â‰¥ 10) |
| **Threshold type** | Fixed or relative only | Adaptive (max of absolute + relative) |
| **Tail latency** | Not checked | Adaptive trimmed mean (k scales with n) |
| **Consistency check** | None | Directionality (informational) |
| **Statistical test** | None | One-sided Mann-Whitney U test |
| **Direction check** | None | Triple condition (prevents false failures) |
| **False positives** | High (2ms noise = "regression") | Low (practical significance override) |
| **False negatives** | High (noise hides real issues) | Low (multiple detection layers) |
| **Uncertainty** | Unknown | Bootstrap confidence intervals |
| **Result types** | Pass/Fail | Pass/Fail/Inconclusive/No Change |
| **Small sample handling** | Unreliable | Adaptive tail metric (stable with n=12) |
| **Multiple testing** | Not addressed | Documented (only 1 p-value test) |

---

## ðŸ’¡ Examples

### Example 1: Multi-Trace Comparison (Recommended)

```bash
# Using the CLI command
perfdiff \
    commit2commit/mock_data/baseline_traces.json \
    commit2commit/mock_data/target_traces.json \
    --output-dir ./test_output

# Or using the module directly
python -m commit2commit.multi_trace_comparison \
    commit2commit/mock_data/baseline_traces.json \
    commit2commit/mock_data/target_traces.json \
    --output-dir ./test_output

# View the results
open test_output/index.html
```

### Example 2: Single Trace Programmatic Usage

```python
from commit2commit.trace_to_trace import gate_regression
import numpy as np

# Your performance measurements
baseline = np.array([100, 102, 98, 101, 99])
target = np.array([110, 112, 108, 111, 109])

# Run statistical analysis
result = gate_regression(baseline, target)

# Check the verdict
if result.passed:
    print(f"âœ… PASS: {result.reason}")
else:
    print(f"âŒ FAIL: {result.reason}")

# Access detailed metrics
print(f"Median delta: {result.details['median_delta_ms']}ms")
print(f"P-value: {result.details['mann_whitney_p']}")
```

### Example 3: CI/CD Integration

```bash
# Install PerfDiff in your CI environment
pip install git+https://github.com/Abdalla-Shawky/PerfDiff.git@v1.0.0

# Run comparison (assumes you have baseline.json and target.json)
perfdiff baseline_traces.json target_traces.json --output-dir ./reports

# Check exit code
if [ $? -eq 1 ]; then
  echo "âŒ Performance regressions detected!"
  echo "ðŸ“Š View detailed report: ./reports/index.html"
  exit 1
elif [ $? -eq 0 ]; then
  echo "âœ… No regressions detected"
else
  echo "âš ï¸  Analysis error occurred"
  exit 2
fi
```

---

## ðŸ§ª Running Tests

```bash
# Install test dependencies
pip install pytest

# Run all tests
cd commit2commit
python -m pytest test_commit2commit.py -v

# Expected output:
# ========================= 52 passed in ~5.0s =========================
```

---

## ðŸ“š Documentation

All documentation is located in the [`docs/`](docs/) folder.

### Core Guides

| Document | Description |
|----------|-------------|
| [TOOL_TECHNICAL_SUMMARY.md](TOOL_TECHNICAL_SUMMARY.md) | ðŸ“Š **NEW!** Complete technical summary (Version 2.0) |
| [STATISTICAL_FIXES_SUMMARY.md](STATISTICAL_FIXES_SUMMARY.md) | ðŸ“Š Original statistical fixes documentation (Version 1.0) |
| [USER_GUIDE.md](docs/USER_GUIDE.md) | ðŸ“– Complete usage guide with examples |
| [TEST_REPORT.md](docs/TEST_REPORT.md) | ðŸ§ª Comprehensive test results (52/52 passing) |
| [MEASUREMENT_GUIDE.md](docs/MEASUREMENT_GUIDE.md) | ðŸ“ Best practices for measurement |

### Feature Documentation

| Document | Description |
|----------|-------------|
| [PREMIUM_UI_COMPLETE.md](docs/PREMIUM_UI_COMPLETE.md) | ðŸŽ¨ Premium UI design details |
| [DATA_QUALITY_FEATURE.md](docs/DATA_QUALITY_FEATURE.md) | ðŸ”¬ Data quality assessment |
| [QUALITY_GATES_GUIDE.md](docs/QUALITY_GATES_GUIDE.md) | ðŸš¦ Quality gates explained |
| [MODES_EXPLAINED.md](docs/MODES_EXPLAINED.md) | âš™ï¸ PR vs Release mode |
| [THRESHOLD_COMPUTATION_EXPLAINED.md](docs/THRESHOLD_COMPUTATION_EXPLAINED.md) | ðŸ“Š Threshold calculation |

---

## ðŸŽ¯ Who Is This Tool For?

**Perfect for:**
- ðŸ”§ **Performance Engineers** - Validating optimizations with statistical rigor
- âœ… **QA Teams** - Setting up reliable performance gates
- ðŸ‘¥ **Engineering Teams** - Tracking performance trends over time
- ðŸ—ï¸ **Platform Teams** - Monitoring system health
- ðŸ”¬ **Data Scientists** - Anyone who values statistical correctness

**Not just another perf tool. This is:**
- âœ… Statistically rigorous (Mann-Whitney U, Bootstrap CI)
- âœ… Battle-tested (52/52 tests passing)
- âœ… Production-ready (comprehensive documentation)
- âœ… Professional UI (stakeholders trust it)
- âœ… False-positive resistant (practical significance override)
- âœ… False-negative resistant (multiple detection layers)

---

## ðŸš¦ Configuration Options

### Quick Reference

| Option | Description | Default |
|--------|-------------|---------|
| `--baseline` | Baseline measurements (required) | - |
| `--target` | Target measurements (required) | - |
| `--out` | Output HTML file (required) | - |
| `--mode` | `pr` or `release` | `pr` |
| `--ms-floor` | Absolute threshold (ms) | `5.0` |
| `--pct-floor` | Relative threshold (fraction) | `0.03` (3%) |
| `--tail-ms-floor` | Tail absolute threshold | `75.0` |
| `--tail-pct-floor` | Tail relative threshold | `0.05` (5%) |
| `--directionality` | Max fraction slower (informational) | `0.70` (70%) |
| `--no-mann-whitney` | Disable Mann-Whitney U test | False |
| `--mann-whitney-alpha` | Significance level | `0.05` |
| `--seed` | Random seed | `0` |

See `--help` for all options.

---

## ðŸ’¡ Quick Tips

### Choosing the Right Mode

**PR Mode** (`--mode pr`): Strict regression gate
- Use for: PRs, feature branches, continuous testing
- Goal: Catch any performance degradation
- Fails if: Median, tail, or Mann-Whitney exceeds thresholds

**Release Mode** (`--mode release`): Equivalence testing
- Use for: Release validation, stable builds
- Goal: Ensure performance hasn't significantly changed
- Fails if: Bootstrap CI for median delta is outside margin

### Sample Size Guidelines

| Samples | Reliability | Use Case |
|---------|-------------|----------|
| 3-5 | Low | Quick checks only |
| 10-20 | Good | Most use cases âœ… |
| 30+ | Excellent | Critical paths, noisy environments |

### Sequential Testing Methodology

âœ… **Correct (AAA BBB):**
```bash
# Run all baseline measurements
for i in {1..10}; do measure_baseline; done
# Then run all target measurements
for i in {1..10}; do measure_target; done
```

âŒ **Incorrect (interleaved):**
```bash
# Don't interleave measurements
for i in {1..10}; do
  measure_baseline
  measure_target
done
```

**Why?** The tool uses independent samples testing (Mann-Whitney U), which assumes samples are collected independently.

---

## ðŸ“Š Project Structure

```
.
â”œâ”€â”€ commit2commit/
â”‚   â”œâ”€â”€ trace_to_trace.py               # Core regression detection logic (single trace)
â”‚   â”œâ”€â”€ multi_trace_comparison.py       # Multi-trace comparison + CLI entry point
â”‚   â”œâ”€â”€ perf_html_report.py             # HTML report generation
â”‚   â”œâ”€â”€ perf_html_template.py           # HTML/CSS/JS template
â”‚   â”œâ”€â”€ constants.py                    # Configuration constants
â”‚   â”œâ”€â”€ test_trace_to_trace.py          # Test suite (52 tests)
â”‚   â”œâ”€â”€ mock_data/                      # Sample test data
â”‚   â””â”€â”€ test_output/                    # Generated test reports
â”œâ”€â”€ STATISTICAL_FIXES_SUMMARY.md        # Statistical fixes documentation
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ docs/                               # Documentation folder (20+ files)
â””â”€â”€ generated_reports/                  # Generated HTML reports (gitignored)
```

---

## ðŸ† Why This Tool?

This isn't just another performance testing tool. It's a **complete solution** built with:

âœ… **Statistical Rigor** - One-sided Mann-Whitney U, bootstrap CI, direction checks
âœ… **Data Quality Focus** - Automatic detection of unreliable measurements
âœ… **Professional UI** - World-class design that stakeholders trust
âœ… **Production Ready** - 52/52 tests passing, comprehensive documentation
âœ… **CI/CD Friendly** - Exit codes, auto-folder creation, reproducible results
âœ… **Transparent** - Shows all thresholds, configurations, and quality metrics
âœ… **No False Failures** - Direction checks prevent failures on improvements
âœ… **No Hidden Regressions** - Dual-threshold override respects tail latency

**Stop guessing. Start measuring with statistical rigor.** ðŸ“Š

---

## ðŸ“„ License

MIT License - Feel free to use in your projects!

---